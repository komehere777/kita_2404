{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task1_0829. 가상 데이터 생성 (generate_data 함수) 후 모델링 및 평가하세요\n",
    "- generate_data 함수는 1000개의 랜덤 시퀀스 데이터를 생성합니다.\n",
    "  - vocab_size: 시퀀스에 사용할 어휘의 크기를 설정합니다. 여기서는 100개의 단어를 사용합니다.\n",
    "  - data: 각 시퀀스는 seq_length=10으로 설정된 10개의 정수(단어 인덱스)로 구성됩니다.\n",
    "  - labels: 각 시퀀스에 대해 0 또는 1의 이진 레이블을 무작위로 할당합니다.\n",
    "\n",
    "- LSTM 기반 분류 모델을 정의하고, 가상 데이터로 학습 및 검증을 수행합니다.\n",
    "- 조기 종료를 통해 학습 중 성능이 더 이상 개선되지 않을 때 학습을 중단합니다.\n",
    "최종적으로 테스트 데이터로 최고 성능의 모델을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 10]) torch.Size([1000])\n",
      "tensor([55, 56, 44, 47, 94, 52, 55, 99, 18,  4]) tensor(0)\n",
      "tensor([20, 85, 40, 98, 29, 54, 70, 26, 17, 32]) tensor(1)\n",
      "tensor([57, 22, 91, 64, 92, 14, 77, 78, 23, 59]) tensor(1)\n",
      "tensor([43, 11, 93, 43, 71,  2, 87, 45, 63, 73]) tensor(0)\n",
      "tensor([59, 68, 69, 28, 74, 43, 62, 57, 31, 27]) tensor(1)\n",
      "tensor([68, 84, 41, 93, 31, 28, 11, 35, 44, 25]) tensor(1)\n",
      "tensor([94, 94, 91, 31, 81, 70, 63, 81, 83, 98]) tensor(1)\n",
      "tensor([27, 98, 53, 26, 38, 76, 74, 92, 31, 93]) tensor(0)\n",
      "tensor([90, 13, 12, 16, 28, 32, 83, 80, 61, 11]) tensor(0)\n",
      "tensor([ 2, 53,  0, 24, 77, 83, 69, 40, 83, 87]) tensor(1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "\n",
    "# 가상 데이터 생성 함수\n",
    "def generate_data(num_samples=1000, seq_length=10):\n",
    "    vocab_size = 100  # 어휘집 크기\n",
    "    data = torch.randint(0, vocab_size, (num_samples, seq_length))\n",
    "    labels = torch.randint(0, 2, (num_samples,))  # 0 또는 1의 레이블\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "data, labels = generate_data()\n",
    "print(data.shape, labels.shape)\n",
    "for i in range(10):\n",
    "    print(data[i], labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate data and labels\n",
    "dataset = TensorDataset(data, labels)\n",
    "dataset_size = len(dataset)\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataset.TensorDataset"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [700, 200, 100])\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.lstm(out, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Training Loss: 0.6948290196332064, Validation Loss: 0.6947515521730695\n",
      "validation loss decreased from inf to 0.694752. Saving the model...\n",
      "Epoch:2, Training Loss: 0.6939650638536974, Validation Loss: 0.6948898094041007\n",
      "Epoch:3, Training Loss: 0.6935347589579496, Validation Loss: 0.6947405338287354\n",
      "validation loss decreased from 0.694752 to 0.694741. Saving the model...\n",
      "Epoch:4, Training Loss: 0.6931178542700681, Validation Loss: 0.694702787058694\n",
      "validation loss decreased from 0.694741 to 0.694703. Saving the model...\n",
      "Epoch:5, Training Loss: 0.6932640427892859, Validation Loss: 0.6948157548904419\n",
      "Epoch:6, Training Loss: 0.6933223496783864, Validation Loss: 0.6946726271084377\n",
      "validation loss decreased from 0.694703 to 0.694673. Saving the model...\n",
      "Epoch:7, Training Loss: 0.6929137815128673, Validation Loss: 0.6946959836142403\n",
      "Epoch:8, Training Loss: 0.6935121010650288, Validation Loss: 0.6946798392704555\n",
      "Epoch:9, Training Loss: 0.6931159008633007, Validation Loss: 0.6953172428267342\n",
      "Epoch:10, Training Loss: 0.6919946535067125, Validation Loss: 0.6946928671428135\n",
      "Epoch:11, Training Loss: 0.6926448345184326, Validation Loss: 0.6949377315385001\n",
      "early stop on epoch 11\n",
      "Accuracy: 58.0%\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(vocab_size=100, embedding_dim=32, hidden_dim=100, num_layers=2, num_classes=2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# train\n",
    "best_val_loss = float(\"inf\")  # 줄어드는지 확인하기 위해 가장 큰값으로 셋팅\n",
    "patience, trials = 5, 0\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for input, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # validation\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input, labels in valloader:\n",
    "            outputs = model(input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(valloader)\n",
    "    print(\n",
    "        f\"Epoch:{epoch+1}, Training Loss: {running_loss/len(trainloader)}, Validation Loss: {val_loss}\"\n",
    "    )\n",
    "\n",
    "    # 조기 종료를 포함한 모델 학습 및 best model 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\n",
    "            f\"validation loss decreased from {best_val_loss:.6f} to {val_loss:.6f}. Saving the model...\"\n",
    "        )\n",
    "        best_val_loss = val_loss\n",
    "        trials = 0\n",
    "        torch.save(model.state_dict(), \"best_model_fashion.pth\")\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f\"early stop on epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# load weight\n",
    "model.load_state_dict(torch.load(\"best_model_fashion.pth\", weights_only=True))\n",
    "\n",
    "# evaluation on testloader\n",
    "correct, total = 0, 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for input, labels in testloader:\n",
    "        outputs = model(input)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
