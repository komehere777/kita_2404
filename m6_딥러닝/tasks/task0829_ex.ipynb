{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task1_0829. 가상 데이터 생성 (generate_data 함수) 후 모델링 및 평가하세요\n",
    "- generate_data 함수는 1000개의 랜덤 시퀀스 데이터를 생성합니다.\n",
    "  - vocab_size: 시퀀스에 사용할 어휘의 크기를 설정합니다. 여기서는 100개의 단어를 사용합니다.\n",
    "  - data: 각 시퀀스는 seq_length=10으로 설정된 10개의 정수(단어 인덱스)로 구성됩니다.\n",
    "  - labels: 각 시퀀스에 대해 0 또는 1의 이진 레이블을 무작위로 할당합니다.\n",
    "\n",
    "- LSTM 기반 분류 모델을 정의하고, 가상 데이터로 학습 및 검증을 수행합니다.\n",
    "- 조기 종료를 통해 학습 중 성능이 더 이상 개선되지 않을 때 학습을 중단합니다.\n",
    "최종적으로 테스트 데이터로 최고 성능의 모델을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 10]) torch.Size([1000])\n",
      "tensor([98, 55, 94,  6, 24, 79, 51, 55, 43, 74]) tensor(1)\n",
      "tensor([ 4, 67, 31, 79, 85, 26, 40, 31, 47, 16]) tensor(1)\n",
      "tensor([48, 75, 26, 54, 70, 36, 90, 51, 68, 23]) tensor(1)\n",
      "tensor([96, 41, 91, 89, 29, 88, 44, 64,  4, 94]) tensor(1)\n",
      "tensor([42, 99, 29, 43, 63, 64, 53, 63,  4, 53]) tensor(0)\n",
      "tensor([76, 95, 88, 31, 44, 46, 64, 53, 82, 35]) tensor(0)\n",
      "tensor([85, 44, 86, 11, 99, 39, 59, 66, 46, 58]) tensor(1)\n",
      "tensor([12, 52, 64, 46, 81, 16, 13,  9, 16, 62]) tensor(0)\n",
      "tensor([80, 72, 93, 95, 65, 58, 65, 86, 92, 43]) tensor(1)\n",
      "tensor([58, 59, 10, 45, 75, 84,  9, 38, 26,  4]) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "\n",
    "# 가상 데이터 생성 함수\n",
    "def generate_data(num_samples=1000, seq_length=10):\n",
    "    vocab_size = 100  # 어휘집 크기\n",
    "    data = torch.randint(0, vocab_size, (num_samples, seq_length))\n",
    "    labels = torch.randint(0, 2, (num_samples,))  # 0 또는 1의 레이블\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "data, labels = generate_data()\n",
    "print(data.shape, labels.shape)\n",
    "for i in range(10):\n",
    "    print(data[i], labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate data and labels\n",
    "dataset = TensorDataset(data, labels)\n",
    "dataset_size = len(dataset)\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataset.TensorDataset"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [700, 200, 100])\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.lstm(out, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Training Loss: 0.6948290196332064, Validation Loss: 0.6947515521730695\n",
      "validation loss decreased from inf to 0.694752. Saving the model...\n",
      "Epoch:2, Training Loss: 0.6939650638536974, Validation Loss: 0.6948898094041007\n",
      "Epoch:3, Training Loss: 0.6935347589579496, Validation Loss: 0.6947405338287354\n",
      "validation loss decreased from 0.694752 to 0.694741. Saving the model...\n",
      "Epoch:4, Training Loss: 0.6931178542700681, Validation Loss: 0.694702787058694\n",
      "validation loss decreased from 0.694741 to 0.694703. Saving the model...\n",
      "Epoch:5, Training Loss: 0.6932640427892859, Validation Loss: 0.6948157548904419\n",
      "Epoch:6, Training Loss: 0.6933223496783864, Validation Loss: 0.6946726271084377\n",
      "validation loss decreased from 0.694703 to 0.694673. Saving the model...\n",
      "Epoch:7, Training Loss: 0.6929137815128673, Validation Loss: 0.6946959836142403\n",
      "Epoch:8, Training Loss: 0.6935121010650288, Validation Loss: 0.6946798392704555\n",
      "Epoch:9, Training Loss: 0.6931159008633007, Validation Loss: 0.6953172428267342\n",
      "Epoch:10, Training Loss: 0.6919946535067125, Validation Loss: 0.6946928671428135\n",
      "Epoch:11, Training Loss: 0.6926448345184326, Validation Loss: 0.6949377315385001\n",
      "early stop on epoch 11\n",
      "Accuracy: 58.0%\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(vocab_size=100, embedding_dim=32, hidden_dim=100, num_layers=2, num_classes=2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# train\n",
    "best_val_loss = float(\"inf\")  # 줄어드는지 확인하기 위해 가장 큰값으로 셋팅\n",
    "patience, trials = 5, 0\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for input, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # validation\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input, labels in valloader:\n",
    "            outputs = model(input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(valloader)\n",
    "    print(\n",
    "        f\"Epoch:{epoch+1}, Training Loss: {running_loss/len(trainloader)}, Validation Loss: {val_loss}\"\n",
    "    )\n",
    "\n",
    "    # 조기 종료를 포함한 모델 학습 및 best model 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\n",
    "            f\"validation loss decreased from {best_val_loss:.6f} to {val_loss:.6f}. Saving the model...\"\n",
    "        )\n",
    "        best_val_loss = val_loss\n",
    "        trials = 0\n",
    "        torch.save(model.state_dict(), \"best_model_fashion.pth\")\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f\"early stop on epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# load weight\n",
    "model.load_state_dict(torch.load(\"best_model_fashion.pth\", weights_only=True))\n",
    "\n",
    "# evaluation on testloader\n",
    "correct, total = 0, 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for input, labels in testloader:\n",
    "        outputs = model(input)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트를 형태소로 분리해야하니 토큰나이저를 사용해야한다.\n",
    "\n",
    "토크나이제이션을 직접 할 일은 없지만 원리를 알면 잘 쓸수 있다.\n",
    "\n",
    "단어는 희소행렬이다 그래서 인베딩을 해야한다.\n",
    "\n",
    "자연어 처리니 lstm을 쓴다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Embedding(vocag_size,embed_dim)\n",
    "- 텍스트 데이터(LSTM 모델): 텍스트 데이터는 이산적인 정수로 표현되므로, 이 정수들을 고차원 벡터로 매핑하는 nn.Embedding 계층이 필요합니다. 이 임베딩 계층은 단어 간의 의미적 유사성을 학습하는 데 유용합니다.\n",
    "\n",
    "- 이미지 데이터(CNN 모델): 이미지 데이터는 이미 공간적 구조를 가진 연속적인 값(픽셀)으로 표현되므로, 임베딩 계층이 필요하지 않습니다. 대신, 합성곱 계층이 이미지의 패턴을 학습하는 데 사용됩니다. 캡쳐, 풀링 계층을 통해 이미지의 공간적 구조를 유지하면서 차원을 줄이거나 특징을 추출할 수 있습니다.\n",
    "\n",
    "LSTM 모델의 경우 텍스트 데이터의 정수 인덱스를 벡터로 변환하기 위해 nn.Embedding- 토치용 임베딩 이 필요하지만, CNN 모델에서는 이미지 데이터를 처리하는 데 이미 직접적인 합성곱 연산이 사용되므로 임베딩 계층이 필요하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.BCEWithLogitsLoss는 PyTorch에서 이진 분류(Binary Classification) 작업을 수행할 때 주로 사용하는 손실 함수입니다. 이 함수는 이진 교차 엔트로피 손실(Binary Cross Entropy Loss)와 시그모이드(Sigmoid) 함수를 결합한 형태로 제공됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로짓(Logit) 값은 모델의 출력값을 의미하며, BCEWithLogitsLoss 함수는 이 출력값에 시그모이드 함수를 적용하여 0과 1 사이의 확률값으로 변환한 뒤, 이진 교차 엔트로피 손실을 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "버트 : 긍부정을 판단하는데 사용된다.\n",
    "\n",
    "개체명 인식 : 개체명을 인식하는데 사용된다.\n",
    "\n",
    "앞의 문맥을 보고 다음 단어가 맞는지 틀리는지 판단.\n",
    "\n",
    "0,1 은 각 학습에 대한 맞고 틀리고 나타내는 것이다.\n",
    "\n",
    "단순 토큰나이제이션(단어로 나누기만 하는 것)을 하면 안쓰는 단어면 문제가 생김"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6938, Val Loss: 0.6976, Val Accuracy: 14.9467\n",
      "Epoch 2, Train Loss: 0.6777, Val Loss: 0.7003, Val Accuracy: 15.0000\n",
      "Epoch 3, Train Loss: 0.6625, Val Loss: 0.7085, Val Accuracy: 14.9867\n",
      "Epoch 4, Train Loss: 0.6384, Val Loss: 0.7197, Val Accuracy: 15.2000\n",
      "조기 종료 발생\n",
      "Test Loss: 0.6979, Test Accuracy: 14.9867\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "\n",
    "# 가상 데이터 생성 함수\n",
    "def generate_data(num_samples=1000, seq_length=10):\n",
    "    vocab_size = 100  # 어휘집 크기\n",
    "    data = torch.randint(0, vocab_size, (num_samples, seq_length))\n",
    "    labels = torch.randint(0, 2, (num_samples,))  # 0 또는 1의 레이블\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "data, labels = generate_data()\n",
    "\n",
    "# 텐서 데이터셋 및 데이터 로더 생성\n",
    "dataset = TensorDataset(data, labels)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - (train_size + val_size)\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# LSTM 모델 클래스 정의\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = hidden[-1, :, :]\n",
    "        return self.fc(hidden)\n",
    "\n",
    "\n",
    "model = LSTMModel(vocab_size=100, embed_dim=50, hidden_dim=100, output_dim=1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# 훈련 함수\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data).squeeze(1)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            outputs = model(data).squeeze(1)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.round(torch.sigmoid(outputs))\n",
    "            correct_predictions = (predictions == labels.unsqueeze(1)).float()\n",
    "            total_accuracy += correct_predictions.sum().item()\n",
    "    return total_loss / len(data_loader), total_accuracy / len(data_loader.dataset)\n",
    "\n",
    "\n",
    "# 조기 종료 로직을 포함한 훈련 및 검증 과정\n",
    "best_val_loss = float(\"inf\")  # float('inf')는 파이썬에서 양의 무한대를 나타내는 방식\n",
    "patience = 3\n",
    "trials = 0\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\"\n",
    "    )\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        trials = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(\"조기 종료 발생\")\n",
    "            break\n",
    "\n",
    "# 테스트 데이터로 최고 모델 평가\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", weights_only=True))\n",
    "test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
