{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0808"
      ],
      "metadata": {
        "id": "aJSEpPR28pVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트의 토큰화\n",
        "**자연어 처리(NLP)에서 토큰화(Tokenization)**\n",
        "- 텍스트 분석, 언어 번역, 감정 분석 등 다양한 NLP 작업을 위해 텍스트를 준비하는 기본 단계입니다.\n",
        "- 토큰화는 텍스트를 단어, 문구, 기호 또는 기타 의미 있는 요소(토큰)로 분해하는 작업을 포함합니다. 이 과정에서 생성된 토큰은 추가 처리 및 분석을 위한 기본 구성 요소가 됩니다.\n",
        "\n",
        "토큰화의 목적\n",
        "- 토큰화의 주요 목적은 텍스트 데이터를 단순화하여 알고리즘이 이해하고 처리할 수 있도록 관리하기 쉽게 만드는 것입니다. 이를 통해 텍스트의 복잡성을 줄이고 일관성을 유지함으로써, 다양한 NLP 작업에서 효율적인 분석과 처리가 가능해집니다. - 단어가 컬럼이 됨\n",
        "\n",
        "토큰화의 유형\n",
        "- 토큰화는 다양한 수준에서 수행될 수 있으며, 각 유형은 특정 `NLP 작업의 요구 사항`에 따라 선택됩니다:\n",
        "  - 단어 토큰화 (Word Tokenization):\n",
        "    텍스트를 개별 단어로 분해합니다.\n",
        "    예: \"ChatGPT is amazing!\" → [\"ChatGPT\", \"is\", \"amazing\", \"!\"]\n",
        "  - 문장 토큰화 (Sentence Tokenization):\n",
        "    텍스트를 개별 문장으로 분해합니다.\n",
        "    예: \"Hello world. How are you?\" → [\"Hello world.\", \"How are you?\"]\n",
        "  - 하위 단어 토큰화 (Subword Tokenization): - 챗쥐피티가 쓰는 방법\n",
        "    단어를 더 작은 의미 단위로 분해합니다. 주로 BPE(Byte Pair Encoding)나 WordPiece 알고리즘을 사용합니다.\n",
        "    예: \"unhappiness\" → [\"un\", \"hap\", \"pi\", \"ness\"]\n",
        "  - 문자 토큰화 (Character Tokenization):\n",
        "    텍스트를 개별 문자로 분해합니다.\n",
        "    예: \"Hello\" → [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
        "\n",
        "토큰화의 과정\n",
        "- 토큰화 과정은 일반적으로 다음 단계로 구성됩니다:\n",
        "  - 텍스트 정규화 (Text Normalization):\n",
        "    모든 텍스트를 소문자로 변환하여 일관성을 유지합니다.\n",
        "    불필요한 구두점과 공백을 제거합니다.\n",
        "    예: \"Hello, World!\" → \"hello world\"\n",
        "  - 구분자 사용 (Delimiter-based Tokenization):\n",
        "    공백이나 구두점을 기준으로 텍스트를 분해합니다.\n",
        "    예: \"hello world\" → [\"hello\", \"world\"]\n",
        "  - 고급 토큰화 기법 (Advanced Tokenization Techniques):\n",
        "    언어의 문법적, 의미적 구조를 고려하여 토큰을 생성합니다.\n",
        "    BPE, WordPiece, SentencePiece 등의 알고리즘을 사용합니다.\n",
        "\n",
        "토큰화의 중요성\n",
        "- 토큰화는 NLP 작업에서 매우 중요한 역할을 합니다. 잘못된 토큰화는 `후속 처리와 분석의 정확도에 큰 영향`을 미칠 수 있습니다.\n",
        "- 반면, 올바른 토큰화는 텍스트 데이터를 효과적으로 전처리하고 분석할 수 있게 합니다."
      ],
      "metadata": {
        "id": "RqscpqNQ8hYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 고급 토큰화 기법\n",
        "언어의 문법적, 의미적 구조를 고려하여 토큰을 생성하는 것은 텍스트의 의미를 더 잘 보존하고, 더 정확한 분석과 처리를 가능하게 하기 위한 고급 토큰화 기법입니다. 이러한 기법들은 단순히 공백이나 구두점을 기준으로 텍스트를 분해하는 것을 넘어, 단어의 의미와 형태, 문장의 구조 등을 이해하여 더 정교한 토큰을 생성합니다.\n",
        "\n",
        "\n",
        "형태소 분석 (Morphological Analysis):\n",
        "- 단어를 구성하는 최소 의미 단위인 형태소를 분석합니다.\n",
        "- 예를 들어, \"cats\"는 \"cat\"과 복수형 접미사 \"s\"로 분해됩니다.\n",
        "- 형태소 분석기는 단어의 어간과 접사(접두사, 접미사)를 인식하고 분리합니다.\n",
        "\n",
        "어간 추출 (Stemming):\n",
        "- 단어의 어간을 추출하여 형태를 단순화합니다.\n",
        "- 예: \"running\", \"runs\", \"ran\" → \"run\"\n",
        "- 포터 스테머(Porter Stemmer)와 같은 알고리즘이 사용됩니다.\n",
        "\n",
        "어근 추출 (Lemmatization):\n",
        "- 단어의 어근을 추출하여 형태를 표준화합니다. 어간 추출보다 더 정교합니다.\n",
        "- 예: \"running\", \"ran\" → \"run\"\n",
        "- 품사 정보를 사용하여 정확한 어근을 찾아냅니다. 예를 들어, \"better\"는 어근 \"good\"으로 변환됩니다.\n",
        "- WordNetLemmatizer와 같은 도구가 사용됩니다.\n",
        "\n",
        "BPE (Byte Pair Encoding): - 챗쥐피티에서 사용, 학습데이터를 보고 자주 나오는걸 기준으로 토큰화 그것이 효율적이기 때문\n",
        "- 자주 등장하는 바이트 쌍을 병합하여 점진적으로 단어를 분해합니다.\n",
        "- 예: \"lowest\"가 \"l\", \"o\", \"w\", \"e\", \"s\", \"t\"로 분해되고, 자주 등장하는 \"lo\", \"we\"가 결합되어 \"low\", \"est\"로 변환됩니다.\n",
        "- BPE는 신경망 번역 모델과 같은 대규모 언어 모델에서 널리 사용됩니다.\n",
        "\n",
        "WordPiece:\n",
        "- BPE와 유사하지만, 서브워드(subword) 단위로 토큰을 생성합니다.\n",
        "- 예: \"unhappiness\" → [\"un\", \"##happiness\"]\n",
        "- 트랜스포머 모델(BERT 등)에서 사용됩니다.\n",
        "\n",
        "SentencePiece:\n",
        "- 언어에 중립적인 방식으로 텍스트를 서브워드 단위로 분해합니다.\n",
        "- BPE와 유사하지만, 문장을 토큰화하는 과정에서 공백을 고려하지 않음.\n",
        "- 예: \"unhappiness\" → [\"un\", \"ha\", \"ppiness\"]\n",
        "- Google의 T5, ALBERT 모델에서 사용됩니다."
      ],
      "metadata": {
        "id": "xBvAVGyB-GKM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASQJFYxT8boi",
        "outputId": "04e7ff03-713c-49df-d5f4-92ed35029323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "원문:  해보지 않으면 해낼 수 없다\n",
            "\n",
            "토큰화:  ['해보지', '않으면', '해낼', '수', '없다']\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from numpy import array\n",
        "\n",
        "# 케라스의 텍스트 전처리와 관련한 함수 불러옴\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "text = \"해보지 않으면 해낼 수 없다\"\n",
        "\n",
        "# 해당 텍스트를 토큰화\n",
        "result = text_to_word_sequence(text)\n",
        "print(\"\\n원문: \", text)\n",
        "print(\"\\n토큰화: \", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tokenizer' 클래스는 텍스트를 정수 시퀀스로 변환하도록 설계\n",
        "- fit_on_texts(docs): 이 메소드는 문장 목록(docs)을 인수로 사용하여 token 개체에서 호출된다. 텍스트 목록을 기반으로 내부 어휘를 업데이트하여 토크나이저가 이러한 텍스트로 작업할 수 있도록 준비한다. 말뭉치의 각 고유 단어에 색인을 할당하고 단어 빈도와 같은 다양한 측정항목을 계산하는 작업이 포함된다.\n",
        "- token.word_counts: 토크나이저를 텍스트에 맞춘 후 word_counts는 키가 입력 텍스트에서 발견된 단어이고 값은 각 단어의 발생 횟수인 OrderedDict를 제공한다. 'OrderedDict'를 사용하면 단어가 텍스트에서 처음 나타나는 순서대로 정렬.\n",
        "- token.document_count: 이 속성은 처리된 총 문서(또는 문장) 수를 표시\n",
        "- token.word_docs: word_counts와 유사한 OrderedDict이지만 단어의 빈도 대신 각 단어가 나타나는 문서 수를 표시\n",
        "- token.word_index: 이 속성은 단어를 고유하게 할당된 정수에 매핑하는 OrderedDict를 제공. 모델에는 숫자 입력이 필요하므로 이는 기계 학습 모델의 텍스트를 벡터화하는 데 필요"
      ],
      "metadata": {
        "id": "btSP9x8xAdFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 빈도수 세기\n",
        "docs = ['먼저 텍스트의 각 단어를 나누어 토큰화합니다.',\n",
        "        '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n",
        "        '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.']\n",
        "\n",
        "# 토큰화 함수를 이용해 전처리합니다.\n",
        "token = Tokenizer()  # 토큰화 함수 지정\n",
        "token.fit_on_texts(docs)  # 토큰화 함수에 문장 적용\n",
        "\n",
        "\n",
        "# 단어의 빈도수와 인덱스 수를 계산한 결과를 출력합니다.\n",
        "# word_counts: 단어 빈도수 odered dictionary 반환\n",
        "print(\"\\n단어 카운트: \\n\", token.word_counts)\n",
        "print(\"\\n문장 카운트: \\n\", token.document_count)\n",
        "print(\"\\n각 단어가 몇개의 문장에 포함되어 있는가:\\n\", token.word_docs)\n",
        "# token.word_index의 출력 순서는 제공된 텍스트 코퍼스(말뭉치)의 각 단어의 빈도에 따라 가장 빈번한 단어부터 인덱스를 부여합니다.\n",
        "# 동일 빈도의 경우는 먼저 등장한 단어가 더 낮은 인덱스를 부여받습니다.\n",
        "print(\"\\n각 단어에 매겨진 인덱스 값:\\n\", token.word_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ44Y_6D_q15",
        "outputId": "5634dcda-3e80-445e-82c4-356073316d77"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "단어 카운트: \n",
            " OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n",
            "\n",
            "문장 카운트: \n",
            " 3\n",
            "\n",
            "각 단어가 몇개의 문장에 포함되어 있는가:\n",
            " defaultdict(<class 'int'>, {'단어를': 1, '텍스트의': 2, '토큰화합니다': 1, '각': 1, '먼저': 1, '나누어': 1, '토큰화해야': 1, '인식됩니다': 1, '단어로': 1, '딥러닝에서': 2, '사용할': 1, '수': 1, '있습니다': 1, '토큰화한': 1, '결과는': 1})\n",
            "\n",
            "각 단어에 매겨진 인덱스 값:\n",
            " {'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화합니다': 7, '단어로': 8, '토큰화해야': 9, '인식됩니다': 10, '토큰화한': 11, '결과는': 12, '사용할': 13, '수': 14, '있습니다': 15}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q. 주어진 docs를 토튼화 해서 아래 사항을 수행하세요."
      ],
      "metadata": {
        "id": "0mnj-cfgDd1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = ['검찰이 제시한 혐의 사실 전부를 재판부가 무죄로 판단하면서 이 회장은 검찰 기소 이후 3년 5개월여 만에 시름을 덜게 됐다.',\n",
        "        '검찰 항소로 2심 재판이 진행될 것이란 전망이 나오지만,']\n",
        "\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(docs)\n",
        "\n",
        "print(\"\\n단어 카운트: \\n\", token.word_counts)\n",
        "print(\"\\n문장 카운트: \\n\", token.document_count)\n",
        "print(\"\\n각 단어가 몇개의 문장에 포함되어 있는가:\\n\", token.word_docs)\n",
        "print(\"\\n각 단어에 매겨진 인덱스 값:\\n\", token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ1yRxycBtz5",
        "outputId": "011591d4-0a48-43d2-c9e3-bbb046c2bec6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "단어 카운트: \n",
            " OrderedDict([('검찰이', 1), ('제시한', 1), ('혐의', 1), ('사실', 1), ('전부를', 1), ('재판부가', 1), ('무죄로', 1), ('판단하면서', 1), ('이', 1), ('회장은', 1), ('검찰', 2), ('기소', 1), ('이후', 1), ('3년', 1), ('5개월여', 1), ('만에', 1), ('시름을', 1), ('덜게', 1), ('됐다', 1), ('항소로', 1), ('2심', 1), ('재판이', 1), ('진행될', 1), ('것이란', 1), ('전망이', 1), ('나오지만', 1)])\n",
            "\n",
            "문장 카운트: \n",
            " 2\n",
            "\n",
            "각 단어가 몇개의 문장에 포함되어 있는가:\n",
            " defaultdict(<class 'int'>, {'5개월여': 1, '판단하면서': 1, '기소': 1, '3년': 1, '덜게': 1, '제시한': 1, '재판부가': 1, '이후': 1, '만에': 1, '무죄로': 1, '검찰': 2, '전부를': 1, '이': 1, '혐의': 1, '사실': 1, '검찰이': 1, '됐다': 1, '회장은': 1, '시름을': 1, '나오지만': 1, '재판이': 1, '항소로': 1, '진행될': 1, '2심': 1, '전망이': 1, '것이란': 1})\n",
            "\n",
            "각 단어에 매겨진 인덱스 값:\n",
            " {'검찰': 1, '검찰이': 2, '제시한': 3, '혐의': 4, '사실': 5, '전부를': 6, '재판부가': 7, '무죄로': 8, '판단하면서': 9, '이': 10, '회장은': 11, '기소': 12, '이후': 13, '3년': 14, '5개월여': 15, '만에': 16, '시름을': 17, '덜게': 18, '됐다': 19, '항소로': 20, '2심': 21, '재판이': 22, '진행될': 23, '것이란': 24, '전망이': 25, '나오지만': 26}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"오랫동안 꿈꾸는 이는 그 꿈을 닮아간다\"\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts([text])\n",
        "print(token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_lhPSL_DzlE",
        "outputId": "c562bc5f-60a3-4df3-bbf9-f75ee498d36f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = token.texts_to_sequences([text])\n",
        "print(\"\\n텍스트, 시퀀스 변환: \\n\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWCLmgK9FgXo",
        "outputId": "6b3e349e-6e8b-4cdf-bddd-d25f0ea5e923"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "텍스트, 시퀀스 변환: \n",
            " [[1, 2, 3, 4, 5, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras의 Tokenizer를 사용하여 텍스트 데이터를 정수 인덱스 시퀀스로 변환한 후, 이를 One-Hot Encoding 형식으로 변환하는 과정은 NLP 모델의 입력 데이터를 준비하는 중요한 단계입니다.\n",
        "\n",
        "Tokenizer를 사용한 텍스트 토큰화\n",
        "\n",
        "word_index:\n",
        "- token.word_index는 각 단어를 고유한 정수 인덱스로 매핑한 딕셔너리입니다. 키는 단어이고, 값은 해당 단어의 인덱스입니다.\n",
        "- 이 딕셔너리의 길이(len(token.word_index))는 말뭉치에 있는 고유 단어의 총 개수를 나타냅니다.\n",
        "\n",
        "word_size:\n",
        "- word_size는 고유 단어의 총 개수에 1을 더한 값입니다. 이는 NLP에서 일반적인 관행으로, \"0\" 인덱스를 포함하기 위해 사용됩니다.\n",
        "- \"0\" 인덱스는 패딩(padding)에 사용되거나, 구현에 따라 알 수 없는 단어를 나타낼 수 있습니다.\n",
        "\n",
        "One-Hot Encoding:\n",
        "- to_categorical 함수는 클래스 벡터(정수 인덱스)를 바이너리 클래스 행렬로 변환합니다.\n",
        "- x는 단어를 나타내는 정수 인덱스의 목록 또는 배열이어야 하며, - to_categorical은 이를 One-Hot Encoding 형식으로 변환합니다.\n",
        "- 각 정수에 대해 해당 인덱스 위치만 1로 설정되고 나머지 위치는 0인 벡터를 생성합니다.\n",
        "\n",
        "num_classes:\n",
        "- num_classes는 총 클래스 수를 지정합니다. 이 경우 어휘 크기(word_size)로 설정되어, One-Hot Encoding에 어휘의 모든 단어에 대한 슬롯과 추가 \"0\" 인덱스가 있는지 확인합니다."
      ],
      "metadata": {
        "id": "uBMBVwziGBjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#인덱스 수에 하나를 추가해서 원-핫 인코딩 배열 만들기\n",
        "# word_size에 1을 추가하는 이유는 keras tokenizer를 사용할 때 0 인덱스를 패딩을 위해 예약하는 관례 때문\n",
        "word_size = len(token.word_index) + 1\n",
        "x = to_categorical(x, num_classes=word_size)\n",
        "print(\"\\n원핫 인코딩 결과: \\n\", x)"
      ],
      "metadata": {
        "id": "zxDSsh1CFi3d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}